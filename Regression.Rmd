---
title: "Regression"
output: pdf_document
---

### Question 8.1

*Describe a situation or problem from your job, everyday life, current events, etc., for which a linear regression model would be appropriate. List some (up to 5) predictors that you might use.*

To predict credit card transactions and the list of predictors could be the following: 
- credit limit
- household income
- whether the bank is the primary financial institution
- total loan balance
- number of mobile/online banking login 


### Question 8.2
*Using crime data from http://www.statsci.org/data/general/uscrime.txt  (file uscrime.txt, description at http://www.statsci.org/data/general/uscrime.html ), use regression (a useful R function is lm or glm) to predict the observed crime rate in a city with the following data:*\
Variable	 	Description
M		        percentage of males aged 14–24 in total state population
So		      indicator variable for a southern state
Ed		      mean years of schooling of the population aged 25 years or over
Po1		      per capita expenditure on police protection in 1960
Po2		      per capita expenditure on police protection in 1959
LF		      labour force participation rate of civilian urban males in the age-group 14-24
M.F		      number of males per 100 females
Pop		      state population in 1960 in hundred thousands
NW		      percentage of nonwhites in the population
U1		      unemployment rate of urban males 14–24
U2		      unemployment rate of urban males 35–39
Wealth		  wealth: median value of transferable assets or family income
Ineq		    income inequality: percentage of families earning below half the median income
Prob		    probability of imprisonment: ratio of number of commitments to number of offenses
Time		    average time in months served by offenders in state prisons before their first release
Crime		    crime rate: number of offenses per 100,000 population in 1960
*Show your model (factors used and their coefficients), the software output, and the quality of fit.*


```{r include=TRUE, echo=FALSE, message=FALSE}
#set working directory
setwd("//")

#packages
library(lmtest) #breusch-pagan test 
library(car) #variance inflation factors test
library(corrplot) #correlation matrix 

#load datafile
crime <- read.csv("uscrime.txt", header = TRUE, stringsAsFactors = FALSE, sep="\t")
summary(crime)
#head(crime)
```

Regression analysis such as linear models, generalized linear models and nonlinear models are considered as parametric approach because the function that describes the relationships between the response and explanatory variables are known. Regression model follows a set of underlying assumptions according to this website (http://people.duke.edu/~rnau/testing.htm#assumptions):
1.Linearity and additivity of the relationship between dependent and independent variables.
2.Statistical independence of the errors (no correlation between independent variables).
3.Homocedasticity of the errors. 
4.Normality of the error distribution. 


There are a number of methods we can use to diagnose if the sample we are using violate those underlying assumptions and many ways to fix those issues. First, let's assume that there is no multicollinearity between the independent variables and run the linear regression model using lm. lm function in R is used to fit linear models. We can also use glm function where it allows response variables to have error distribution models other than a normal distribution - for instance, binomial, inverse gaussion, poisson and many more. 

The linear model (assuming no multicollinearity) shows that the following results:
- adjusted R-squared is 0.7078 which is quite good (R squared is used to assume all independent variables explain the variation in the dependent variables. But adj.R-squared tells the % of variation by the independent variables that actually affect the the dependent variable).
- F-value is highly significant with p-value <= 0.05 (F-Test is used to see whether any of the independent variables used in the linear model are significant). This implies that all the independent variables explain the dependent variable, crime. 
- The F-value is a good news, but if we look at p-value for individual coefficients, there are only a few variables that are significant at 95% significance level e.g. M, Ed, Ineq, Prob (which are displayed with *). 

```{r include=TRUE, echo=FALSE, message=FALSE}
# fitting the linear model, assuming no multicollinearity
lm_no_mc <- lm((Crime)~M+So+Ed+Po1+Po2+LF+M.F+Pop+NW+U1+U2+Wealth+Ineq+Prob+Time, data=crime)
summary(lm_no_mc)
```
There are 4 plots generated from the linear model: 
- Residuals vs Fitted: mainly used to investigate if linearity and homoskedasticity holds & presence of outliers. 
- Normal Q-Q: a simple way to check if points lie approximately on the line & if not, the residuals & errors are not Gaussian. 
- Scale-Location: simplified version to check for homokedasticity and 2 main things to check are 1) red line is approximately horizontal which can show the average magnitude of the standardized residuals isn't changing much, and 2) spread around the red line are not too vary. 
- Residuals vs. Leverage: this helps to detect extreme outliers, and we may want to check individually to see if there's any needs to remove the outliers and whether it would help to improve/degrade the results of the the linear model. 


```{r include=TRUE, echo=FALSE, message=FALSE}
par(mfrow=c(2,2))
plot(lm_no_mc)
```
We can analyze and determine the existence of heteroskedasticity based on the "Residuals vs. Fitted" plot. But sometimes, this may takes years of experience to conclude just by looking at a visualization. Another method is to test using Breush-Pagan test.Breusch-Pagan test is used to check whether the variance of the errors from a regression is dependent on the values of the independent variables. It is also known as chi-squared test.

The result shows that we fail to reject the null hypothesis (p-value > 0.05) which means there is no heteroskedasticy. 

```{r include=TRUE, echo=FALSE, message=FALSE}
#test for violation on homocedasticity
bptest(lm_no_mc)
```

Variation Inflation Factors (VIF) can be used to detect multicollinearity in the model.A VIF between 5 and 10 indicates high correlation, and that may be problematic. We can also check from the correlation matrix, if there are any independent variables that are highly correlated to each other. We can choose either one of the mutiple highly correlated variables to use and test in the linear model, or we can use PCA to address this issue. 

For the purpose of this assignment, let's just choose independent variables that are correlated with dependent variable and yet not strongly correlated between/among themselves that would cause a multicollinearity issue. 

Based on the corrplot results, the correlated independent variables with Crime (correlation >= 0.30) are Ed, Po1, Po2, Pop, Wealth, Prob. From the VIF and corrplot, we know that Po1 and Po2 are strongly correlated and would cause a multicollinearity issue. It is not surprising since both indicate similar data records (it's just Po1 = per capita expenditure on police protection in 1060 and Po2 for 1959). We will just use Po1 and drop Po2 from the linear model. 


```{r include=TRUE, echo=FALSE, message=FALSE}
#test for violation on multicollinearity
car::vif(lm_no_mc)

```

```{r include=TRUE, echo=FALSE, message=FALSE}
#plot correlation matrix 
corrplot(cor(crime), is.corr=FALSE, method="number", type="upper", number.cex=0.7)

```
The final linear model shown below is overfit as we haven't taken into consideration of cross-validation. But for now, we have at least resolve the issues of multicollinearity. While the adjusted R-squared is low, but this doesn't mean that the model is not useful because it may be hard to explain human behavior and it is just harder to predict than things like manufacturing processes.

- Adjusted R-squared: 49.91 of the variation in the model can be explained 
- F-Test: p-value <= 0.05, and this means we can just null hypothesis and conclude that the model provides a better fit than the intercept-only model.

```{r include=TRUE, echo=FALSE, message=FALSE}
lm_corrected_mc <- lm((Crime)~Po1+Wealth+Prob, data=crime) #drop Ed and pop because not significant
summary(lm_corrected_mc)

cat("\n")
round(coef(lm_corrected_mc),5)
cat("\n")

#test for violation on multicollinearity
car::vif(lm_corrected_mc)
```

We can also do log-transformation on all variables, so we can explain the changes/effects in terms of percentages. For instance, a 1% increase in Po1 would increase the crime by 1.13%. A negative sign on a coefficient shows the negative effect, e.g. 1% increase in Wealth, would decrease the crime rate by 0.85%.

```{r include=TRUE, echo=FALSE, message=FALSE}
lm_corrected_mc_log <- lm(log(Crime)~log(Po1)+log(Wealth)+log(Prob), data=crime) #drop Ed and pop because not significant
summary(lm_corrected_mc_log)

cat("\n")
round(coef(lm_corrected_mc_log),5)
cat("\n")

#test for violation on multicollinearity
car::vif(lm_corrected_mc_log)
```

